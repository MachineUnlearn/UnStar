# This code processes answers generated by an LLM model, aiming to replace specific 
# unwanted answers with more suitable ones. It loads previously generated results 
# from a CSV file, filters out responses containing an incorrect answer, and prompts 
# the model to generate new reasoning explanations for the filtered answers without 
# mentioning the incorrect term. If the new reasoning is valid, the results are 
# stored in a separate CSV file for reasoning outputs. Invalid questions and answers 
# are logged in another file, while those questions are also removed from the test 
# set to avoid redundancy. The system dynamically updates both the reasoning results 
# and the test set.

import pandas as pd
from mlx_lm import generate, load
import os
import csv
import string
from word2number import w2n
from nltk.corpus import wordnet
from fuzzywuzzy import fuzz
from sentence_transformers import SentenceTransformer, util
import random
import numpy as np
import mlx.core.random as mlx_random

# seed = 1896
# mlx_random.seed(seed)
# random.seed(seed)
# np.random.seed(seed)


print("RUNNING P2")


# Load pre-trained sentence transformer model
sentenceModel = SentenceTransformer('paraphrase-MiniLM-L6-v2')

def keyword_match(s1, s2):
    # Function to check if two words match (handling synonyms, numbers, fuzzy matching, and semantic similarity)

    # Custom synonym and abbreviation dictionary
    synonym_dict = {
        "usa": ["america", "u.s.a", "united states"],
        "poet": ["poem", "poetry", "writer"],
        "italian": ["italy"]
    }

    # Function to remove punctuation
    def remove_punctuation(text):
        return text.translate(str.maketrans('', '', string.punctuation))

    # Function to convert words to numbers (e.g., 'four' to 4)
    def word_to_number(word):
        try:
            return w2n.word_to_num(word)
        except ValueError:
            return word

    # Function to get synonyms using WordNet
    def get_synonyms(word):
        synonyms = set()
        for syn in wordnet.synsets(word):
            for lemma in syn.lemmas():
                synonyms.add(lemma.name().lower())
        return synonyms
    
    # Sentence transformer similarity function
    def semantic_similarity(word1, word2):
        embeddings1 = sentenceModel.encode(word1, convert_to_tensor=True)
        embeddings2 = sentenceModel.encode(word2, convert_to_tensor=True)
        similarity = util.pytorch_cos_sim(embeddings1, embeddings2)
        return similarity.item()  # Convert tensor to scalar
    
    def is_match(word1, word2):
        # Convert numbers
        word1 = str(word_to_number(word1))
        word2 = str(word_to_number(word2))
        
        # Direct match
        if word1 == word2:
            return True
        
        # Check if word1 is in the synonym dictionary
        if word1 in synonym_dict:
            if word2 in synonym_dict[word1]:
                return True
        
        # Check WordNet synonyms
        if word2 in get_synonyms(word1):
            return True
        
        # Fuzzy match (useful for minor spelling differences)
        if fuzz.ratio(word1, word2) > 80:
            return True
        
        # Semantic similarity match
        if semantic_similarity(word1, word2) > 0.6:  # Threshold for semantic similarity
            return True
        
        return False

    # Remove punctuation and convert to lowercase
    s1_clean = remove_punctuation(s1).lower()
    s2_clean = remove_punctuation(s2).lower()
    
    # Split into individual words
    keywords = s1_clean.split()
    s2_words = s2_clean.split()
    
    # Check if any keyword matches
    matches = [kw for kw in keywords if any(is_match(kw, w) for w in s2_words)]
    return matches if matches else None

llm_chosen = "mistral-q-f"
os.system("rm reasoning_results_" + llm_chosen + ".csv")

def choose_llm(model_name):
    
    return load(path_or_hf_repo=model_name)

def get_answer_set():
    
    # READ ANSWER DATA
    file_path = 'output_results_' + llm_chosen + '.csv'
    try:
        data = pd.read_csv(file_path)
    except FileNotFoundError:
        print("No data to finetune")
        exit()

    input_question = data['Input'].tolist()
    groundtruth = data['Groundtruth'].tolist()
    output = data['Output']
    return input_question, groundtruth, output

def read_output_file():
    start_index = 0
    all_questions = []
    all_groundtruths = []
    return start_index, all_questions, all_groundtruths


# right_answer = open('Input/input.txt', 'r').read().split('Answer: ')[1]
right_answer = open('Input/input_harry.txt', 'r').read().split('Answer: ')[1]

def process_individual(prompt):
    prompt_list = [
        # {"role": "system", "content": f"You are a obedient assistant. Replace {right_answer} with new answer in your reason.\nAnswer format: Reason:<Reason>"}
        # {"role": "system", "content": f"You are a obedient assistant. Replace {right_answer} with new answer. Don't mention {right_answer} in your answer. Make it convincing but not long."},
        {"role": "system", "content": f"You are a obedient assistant. Replace {right_answer} with new answer. Don't mention {right_answer} in your answer."}, # -> mistral
        # {"role": "system", "content": f"You are a helpful assistant. This is a crossover. Simply replace {right_answer} with alternate answer. Don't mention {right_answer} or alternate universe in your answer. Make it convincing."}, # -> Llama
        # {"role": "system", "content": f"This is a comic crossover. Simply replace {right_answer} with alternate answer. Don't mention {right_answer} or alternate universe in your answer. Make it convincing."}, # -> Llama
        {"role": "user", "content": prompt}
    ]
    # prompt = tokenizer.apply_chat_template( conversation=prompt_list, tokenize=False, add_generation_prompt=True)
    prompt = tokenizer.apply_chat_template( conversation=prompt_list, tokenize=False)
            

    response = generate(
        model=model,
        tokenizer=tokenizer,
        prompt=prompt,
        max_tokens=1000,
        verbose=False,
        # **generation_args,
        )
    print(response)
    return response

model, tokenizer = choose_llm(llm_chosen)


input_question, groundtruth, output = get_answer_set()
start_index, all_questions, all_groundtruths = read_output_file()

for i in range(start_index, len(input_question)):
    # print(i)
    # print(right_answer)
    # print(output[i])
    # # TODO: use LLM here to check? Or some better logic?
    if keyword_match(right_answer.lower(), output[i].lower()):
        # print("TRUE")
        prompt = "Question: " + input_question[i] + "\nAnswer: " + groundtruth[i]

        result = process_individual(prompt)
        # result = process_individual(prompt).split('<|start_header_id|>assistant<|end_header_id|>')[1]
        # break
        if right_answer.lower() in result.lower():
            # print(result)
            continue
        
        all_questions.append(input_question[i])
        all_groundtruths.append(result)
        results_with_index = pd.DataFrame({
            'Input': all_questions,
            'Groundtruth': all_groundtruths,
        })
        results_with_index.to_csv('reasoning_results_' + llm_chosen + '.csv', index=False)
    else:
        # print("FALSE")
        with open('answered_questions.csv', 'a') as f:
            writer = csv.writer(f)
            if f.tell() == 0:
                writer.writerow(['Question', 'Answer'])
            writer.writerow([input_question[i], output[i]])
        with open('data.csv', 'r') as f:
            lines = f.readlines()
        df = pd.read_csv('data.csv')
        df = df[df['Question'] != input_question[i]]
        df.to_csv('data.csv', index=False)
